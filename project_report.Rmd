---
title: "Project-CS510"
author: "Lucas Jeay-Bizot"
date: "13/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Simulating Two Models of Human Neural Correlates of Volition

## Background

### Free Will

Free will is a widely disputed topic, there is no consensus on its definition amongst neither philosophers [1] nor laypeople [2]. However, the ramifications of the definition of free will heavily impacts our societies. Definitions and understanding of free will are often implicitly assumed in legal and moral matters without further considerations. Most philosophical debates have revolved around the metaphysical nature of free will, such as questions surrounding the causal role of an agent in an action. Neuroscientists and philosophers have been at odds disputing the importance of a scientific investigation of free will. Free will as a broad concept to study without some theoretical commitments.
Here, we propose to study instead the closely related phenomenon of volitional action. We propose to investigate two models of the electrophysiological correlates of spontaneous actions. Such actions, in laboratory settings, often translate to simple hand movements performed whenever the participants desire so. The electrophysiological correlates of these spontaneous hand movements are usually recorded using electroencephalograms (EEG). EEG measure the scalp’s electrical current. This current is generated by neuronal activity in cortical regions. The activity of cortical neurons can thus be picked up as a signal by electrodes placed on the scalp. This measurement of the brain activity takes the shape of a signal – a wavy line instantiating different frequencies. An EEG recording, typically comprises multiple channels, in our case we recorded activity from 64 channels over the scalp. This means that for each electrode placed on the scalp we have one signal. 

### The Readiness Potential

Recent developments in the cognitive sciences enabled the discovery of the readiness potential (RP) [3]. The RP is a slow negative deflection over motor regions in the brain’s electrical signal occurring about 1-2 seconds prior to self-initiated actions. This deflection has been shown to precede the moment of a subject’s awareness of the decision to move [4]. In light of these results, it can been hypothesised that the RP represents the pre-conscious mechanisms of the decision to act. As such, many definitions of free will would fail to attribute free will to simple self-initiated motor actions.

### A Stochastic Accumulator Model of the RP

A more recent interpretation of the RP [5] is that it results from the averaging of trials in a task containing an event (self-initiated actions) triggered by a threshold crossing at the level of the brain’s electrical activity. Indeed, this account proposes that a decision to act is more likely to occur when the participants’ electrical amplitude over the motor cortex is low. As a result, averaging data from the moments leading to a movement will consistently reveal a slow negative deflection to the threshold.
In this project we propose to lay the foundations to experimentally tell both models of the RP apart.


## The Project

### The Time-locked Approach

Analysis of events, such as self-initiated action, has traditionally been studied using a time-locked approach. That is, only the moments closely preceding or following are in the focus of investigation. Specifically, the RP is a signal derived by averaging, usually over 40 trials, the activity in the 3 seconds preceding movement onset. If the RP reflects mechanisms described by the stochastic accumulator model of the RP, then such an approach will generate a slow negative deflection as an artefact of the time-locking. Indeed, if a movement is more likely to occur when the signal is in a low negative state, then looking at the moments preceding the movement a convergence to these low states will be observed as a slow negative deflection. In this project, we thus propose to move away from a time-locked approach and adopt a time-unlocked analysis.

### Forecasting

We propose to generate forecast maps of events occurring in the future based on entire timeseries of brain signal. We will use a training dataset of either real or simulated EEG data, containing baseline EEG data - equivalent to a participant fixating a cross at the centre of a screen - and events - equivalent to a participant performing a self-initiated button press.
We will evaluate each and every timepoint of this timeseries. For each timepoint we will look about 3 seconds into the future whether an event happened or not. Using this information, we will tune a forecasting map made up of conditional probabilities of events occurring in the future based on the current state of the system. 
As such, we will have forecast maps of the probabilities of an event occurring in the future given the current state of the signal, in this project we take the current state of the signal to be the first derivate of the signal. We decided so, based on the fact that the RP is a slow negative deflection, indicating that the signal’s derivative might carry the most information about the RP. These forecasts can be thought of as weather forecast maps, inputting the current signal’s first derivative one can extract the probability of an event occurring in the near future using these forecast maps.


### Simulating Two Approaches

In this project we propose to simulate EEG data using pink noise values, mean and variance of real EEG signal to generate baseline data. We then propose to appose events on these signals according to the two above-mentioned differing models of the RP. In the first case, that we will call model A, we will add the same signal (an RP from real EEG data) at different randomly selected intervals. In the second case, that we will call model B, we will search for a threshold that is crossed exactly as many times as the number of events that we desire to have in our data.
Both interpretations of the RP are still thriving. Here, we propose to simulate EEG data according to each model and proceed to compare them using a novel analysis tool that will enable us to generate forecast maps. This would constitute solid predictions of what real EEG data forecast maps should look like according to each model. This paradigm would thus enable us to test predictions borne out by both models in an adversarial manner. Meaning that generating such maps from our simulations will enable us to be able to tell apart the two models in a testable empirical fashion.


### Simulation as a Power Analysis

Running the forecast matrix on our simulated data will enable us to estimate how many events will be necessary in order for the forecast matrix to stabilize. We can do so by running multiple runs of the forecast, each with more events, and compare the distance between the matrices every time.
Such an estimate will enable us to gauge how much data from each participant will be required for the analysis to uncover a stable forecast map.


## Methods

### The Task and the Data

The standard Libet task [4] would require participants to flex their wrists whenever they feel like it. In an experiment, we had a participant perform a similar task – replacing wrist flexing with button presses. These button presses are what we call events, and what we call RP will be the brain’s electrical activity in the 4 seconds preceding the button press. We opted for 4 seconds in order to be sure to encompass the RP that starts approximately 1-2 seconds before the button press.
Please note that the EEG recording, and analysis was performed in MATLAB outside the scope of this project. We generated CSV files from MATLAB containing resting EEG data information from the participant as well as the RP that we later imported into R (these can be found in the data/ folder) .

### The Simulations

From our single participant we thus extracted brain activity off-task, i.e. when the participant was not performing any tasks – simply staring at the screen. In MATLAB we extracted the power spectrum beta values for each channel in our participant. We further extracted the signal mean and variance at each channel. This resulted in a CSV file (“data/EEG_value_subject008.csv”) containing 3 columns (beta values, mean, variance) and 64 rows (each channel).
Using the power spectrum beta values of the real resting EEG data, we generated in “src/EEG_simulator.R” normally distributed vector (using rnorm()) and modified its beta values according to the real EEG values. We modified the beta values of this random signal by using the function “src/change_beta.R”. This function brought our random vector, which is white-noise, into the power frequency domain using fast Fourier transform, and modified it’s beta value by inducing a slope with our desired beta values on its frequency distribution. The function then brought it back into the time domain using an inverse fast Fourier transform (available in the package “pracma”).
The resulting signal was then scaled (still in src/EEG_simulator.R) to the real EEG data, using the mean and variance from the real EEG data.
The result is a set of 64 simulated resting EEG signals (in matrix format), one for each channel.

### Model A

To simulate data for model A, in “modelA_generator.R” we generated random positions according to the desired number of events. This desired number of events is determined based on the inputs. It corresponds to (simulation_duration*60)/numEvents_perMinute, where simulation_duration is the desired amount of simulated data in seconds and numEvents_perMinute is the desired number of events per minutes.
We then created a 0 vector for each channel and pasted the RP data from “data/RP_data_subject008” into each corresponding vector-channel. This vector was then added to the simulated data.
The data from all 64 channels was then collapsed into a single signal by using a spatial filter. The spatial filter was determined as a function of the channels with the most change in the signal’s first derivative in the second preceding the event.
The resulting signal was then filtered using a Butterworth low-pass filter of 10 Hz in order to remove any noise that happens within less than 0.1 second. Since our target signal is a slow signal which can last up to 2 seconds, such low-pass filtering should only improve the signal-to-noise ratio of our data.
Labels for where the events were placed was added in an additional row to the data.
As a result each event is preceded by an underlying signal, the RP, as described by model A.

### Model B

To simulate data for model B, in “modelB_generator.R” we first filtered the simulated data from “EEG_simulator.R” using the same spatial filter as described in model A.
We then ran a search function to identify an value of the signal’s first derivative such that the signal crosses this value from above only as many times as desired number of events.
Without manipulation of the data, these event crossings were then labelled in an additional row to the data.
As a result, the events mimic a threshold crossing triggering behaviour. In our simulated dataset, whenever a specific threshold is crossed, a movement is triggered. This is the behaviour described by model B.


### The Forecasting

We then generate a forecast matrix of conditional probabilities of events happening in the future. Our forecast matrix will generate forecast for as many timepoints in the future as indicated by the input variable “numFuture”. The width of a timepoint in this context is determined by another input variable “timeBins_perSecond”, which indicates how many desired timepoints there should be in a second. For instance, for timeBins_perSecond = 20 and numFuture = 60, then one would get a forecast map spanning the next 3 seconds.
In order to generate a forecast matrix, in “forecast_matrix.R” we took the labelled simulated time series from model A or model B. We generated as many variable bins as required by the input variable “varBins”. The variable in our case being the signal’s first derivative. Variable bins were determined based off of percentiles, such that, for any two variable bins, the signal falls the same number of times (give or take one) in each of these two bins. As such, knowing which variable bin the signal is in, gives us a measure relative to the entire signal’s timeseries.
Using these variable bins, we then look at each time point in the timeseries. If, for the signal being in bin b, there was an event labelled at t time points in the future, then we added a one at position (b,t) in the forecast matrix. It resulted in a “varBin” (b) by “numFuture” (t) matrix filled with the count in each cell of how many times the timeseries attained a certain variable value (b) and an event happened in the future at time (t).
In order to generate conditional probabilities, we next needed to divide each columns of this matrix by a vector containing, for each variable bin (b), the total number of times the timeseries attained this value (regardless of whether or not an event happened). This resulted in our forecast maps. In which each cell contains a conditional probability of events happening in the future given the current first derivative of the signal.

### The Power Analysis

As we also aim to gather an estimate of how much data will be needed to generate a stable forecast map. We here use a simple quantifiable method to measure the difference between two forecast maps. We propose to take the squared difference between each cell of two forecast maps of the same size. Our goal here is not to compare forecast maps across models. But rather to compare the stability of our forecast maps within models, as we increase the number of events. This should give us an idea of how much data is needed to achieve stable forecast maps using real EEG data.
We perform this power analysis using “data_length_finder.R”, this analysis option runs a comparison of two timeseries of same length each generated by model A. At each time step, the simulation duration is increased in increments of its initial value. The result is then plotted with distance between maps on the y-axis and number of increment steps on the x-axis. A cut-off point that could be considered as enough data for the maps to stabilize would be one where the distances converge to 0. 


## The Results

### The Power Analysis

Result from the power analysis for model A and B can be seen in the plots below (Figure 1 and Figure 2).

```{r, echo=FALSE}
project_path <- getwd()
result_path <- paste(project_path, "/results/", sep = "")
plot(as.matrix(read.table(paste(result_path, "DataLentgh_modelA_result_015.Rdata",sep = ""))), xlab = "Number of Events", ylab = "Difference between Forecast Maps", ylim = c(0,0.2), main = "Figure 1")
```

```{r, echo=FALSE}
project_path <- getwd()
result_path <- paste(project_path, "/results/", sep = "")
plot(as.matrix(read.table(paste(result_path, "DataLentgh_modelB_result_015.Rdata",sep = ""))), xlab = "Number of Events", ylab = "Difference between Forecast Maps", ylim = c(0,0.2), main = "Figure 2")
```

Both models were run starting with 18 events and then incremented by further 18 events at each step. 15 runs were compared in each figure, the starting run comprising 18 events and the final run comprising 270 events. This choice of increment was made for computational power reasons.
Graphically, in model A it seems that after 8 steps the comparison stabilized. In model B it seems that such stabilization is achieved after 3 steps.

To further investigate when the change in matrix difference started to decay we plotted the first derivative of each power analysis (Figure 3 and Figure 4)

```{r, echo=FALSE}
project_path <- getwd()
result_path <- paste(project_path, "/results/", sep = "")
plot(diff(as.matrix(read.table(paste(result_path, "DataLentgh_modelA_result_015.Rdata",sep = ""))), lag = 1), xlab = "Number of Events", ylab = "Difference between Forecast Maps", ylim = c(-0.2,0.2), main = "Figure 3")
```


```{r, echo=FALSE}
project_path <- getwd()
result_path <- paste(project_path, "/results/", sep = "")
plot(diff(as.matrix(read.table(paste(result_path, "DataLentgh_modelB_result_015.Rdata",sep = ""))), lag = 1), xlab = "Number of Events", ylab = "Difference between Forecast Maps", ylim = c(-0.2,0.2), main = "Figure 4")
```

Here, it seems clear that very little change happens after one increment step in both models.

Of course this stabilization might be due to the fact that an 18 event increment becomes proportionally less significant once the number of events becomes big. To try to account for this possibility we can plot the results using a log axis for the increments (Figure 5 and Figure 6):

```{r, echo=FALSE}
project_path <- getwd()
result_path <- paste(project_path, "/results/", sep = "")
plot(as.matrix(read.table(paste(result_path, "DataLentgh_modelA_result_015.Rdata",sep = ""))), xlab = "Number of Events", ylab = "Difference between Forecast Maps", ylim = c(0,0.2), main = "Figure 5", log = "x")
```

```{r, echo=FALSE}
project_path <- getwd()
result_path <- paste(project_path, "/results/", sep = "")
plot(as.matrix(read.table(paste(result_path, "DataLentgh_modelB_result_015.Rdata",sep = ""))), xlab = "Number of Events", ylab = "Difference between Forecast Maps", ylim = c(0,0.2), main = "Figure 6", log = "x")
```

Using log plots, it appears less clear whether the two maps really converge towards similar values. However neither log plots (Figure 5 and Figure 6) seem to represent a constant rate of change (straight line), rather the curvature seems to indicate that the two maps do converge. 
We might need to use a cluster to run this analysis using more increment steps in order to evaluate if there is really a trend towards the convergence of the two forecast maps. 
Further developments of this project could also include finding a better tool to compare the two forecast matrix.
However, at this early stage, the results from this power analysis seem to be more promising than not. Both indicating that acquiring 144 (8 times 18) events would be enough for the forecast maps to stabilize under either model. 

### The Comparison of the Models

The comparison between the two models remains mainly qualitative
Using the following default parameters, one can generate two distinct forecast maps using either model A or model B (Figure 7 and Figure 8). These default parameters include the sampling rate (Srate), which represents the number of samples per second, the number of subjects (numSubjects), the number of desired variable bins (varBins), the number of desired forecasted future timepoints (numFuture), the length of the simulation in seconds (simulation_duration), the desired number of events per minute (numEvent_perMin), the desired minimal spacing between any two events in seconds (spacing), the desired number of timebins per seconds in the forecast map (timeBins_perSecond) and the signal to noise ratio for model A (coef_SNR).

```{r, echo=FALSE}
# initialize empty parameters data.frame

parameters <- c(0, 0, 0, 0, 0, 0, 0, 0, 0)
names(parameters) <- c("Srate", "numSubjects", "varBins", "numFuture", "simulation_duration", "numEvent_perMin", "spacing", "timeBins_perSecond", "coef_SNR")
parameters <- as.data.frame(t(parameters))

# set default parameters

parameters$Srate <- 512                  # sampling rate
parameters$numSubjects <- 1              # number of subjects
parameters$varBins <- 30                 # number of possible variable states (for forecast)
parameters$numFuture <- 60               # number of future time points to be forecasted
parameters$simulation_duration <- 360    # duration of the simulated data (for each subject) in seconds
parameters$numEvent_perMin <- 3          # desired number of events per minute
parameters$spacing <- 6                  # desired minimal spacing between each event
parameters$timeBins_perSecond <- 20      # desired size of the timepoints in the forecast map
parameters$coef_SNR <- 1                 # signal to noise ratio of the RP signal in model A

parameters
```

Here, the choice for Srate was based on the sampling rate of real EEG data, numSubjects was set to 1 as this parameter is not yet fully implemented in all the analysis (most of the code can only process one subject at time yet), varBins was arbitrarily set to 30, numFuture was set to 60 to allow for forecasting up to 3 seconds in the future (thus encompassing the RP), simulation_duration was set to 360 for computationability, numEvent_perMin was set to 3 to mimic a real setting, spacing was set to 6 to avoid an overlap of RPs, timeBins_perSecond was set to 20 to allow increase forecasting accuracy and coef_SNR was set to 1 such that the signal (RP) and the noise would mimic real EEG data.

These default parameters yield the following forecast maps, respectively for model A and model B:

```{r, echo=FALSE}
project_path <- getwd()
result_path <- paste(project_path, "/results/", sep = "")
heatmap(as.matrix(read.table(file = paste(result_path, "modelA_result001", ".Rdata", sep = ""))), Rowv = NA, Colv = NA, scale = "none", ylab = "Signal's First Derivative", xlab = "Time in the Future", main = "Figure 7")
```


```{r, echo=FALSE}
project_path <- getwd()
result_path <- paste(project_path, "/results/", sep = "")
heatmap(as.matrix(read.table(file = paste(result_path, "modelB_result001", ".Rdata", sep = ""))), Rowv = NA, Colv = NA, scale = "none", ylab = "Signal's First Derivative", xlab = "Time in the Future", main = "Figure 8")
```

These seem to be quite different, with a polarity appearing in the first map and and gradient with no polarity in the second.
Model A yields a forecast map with two relatively consistent predictions for movement over time, with two initial values that present higher conditional probabilities of event (around 3-4 and 26-27). This means that very steep up- or down- going slopes of EEG amplitudes indicate a higher probability of movement over the next 3 seconds. On the other hand, model B yields a a threshold at aroung value 25 with a high concentration of probabilities in the near future that diffuses over time. As a result, these two forecast maps present very distinctive features.

## Discussion

These results are preliminary, and many improvements are yet to be made. However, they provide a strong picture of how the two models of the RP could be told apart experimentally. Indeed, the distinctive features that both forecast maps display, indicate that forecast maps generated from empirical data might also display distinctive features that might pertain to either of our models.
Future improvements to improve the forecasting accuracy will be to include multiple input variable. Here, the forecasts were generated using the first derivative of the spatially filtered signal. Future steps will include the signal's amplitude as a new axis of forecasting. This will result in 3D forecasting maps. These will be more fitted to study a phenomenon like the RP as it will encompass information about the amplitude and the direction of the signal.
Result from the power analysis are also promising indicating that the amount of data necessary to stabilize the forecast maps is within obtainable within reason. Indicating that a future step for this project will be to collect real EEG data to enable a comparison between the empirical data and our two model's predictions.

Overall this project presents a promising start to a novel analysis tool in the study of volition.

## References

[1] Bourget, D., & Chalmers, D. J. (2014). What do philosophers believe? _Philosophical Studies, 170_(3), 465–500. https://doi.org/10.1007/s11098-013-0259-7

[2] Nichols, S. (2011). Experimental Philosophy and the Problem of Free Will. _Science, 331_(6023), 1401–1403. https://doi.org/10.1126/science.1192931
[3] Kornhuber, H. H., & Deecke, L. (1965). Hirnpotentialanderungen bei Willkarbewegungen und passiven Bewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. _Pflagers Archiv Fur Die Gesamte Physiologie Des Menschen Und Der Tiere, 284_(1), 1–17. https://doi.org/10.1007/BF00412364

[4] Libet, B., Gleason, C. A., Wright, E. W., & Pearl, D. K. (1983). Time of conscious intention to act in relation to onset of cerebral activity (readiness-potential). The unconscious initiation of a freely voluntary act. _Brain: A Journal of Neurology, 106 (Pt 3)_, 623–642. http://www.ncbi.nlm.nih.gov/pubmed/6640273

[5] Schurger, A., Sitt, J. D., & Dehaene, S. (2012). An accumulator model for spontaneous neural activity prior to self-initiated movement. _Proceedings of the National Academy of Sciences of the United States of America, 109_(42), 2904–2913. https://doi.org/10.1073/pnas.1210467109


_Project report and repository by Lucas Jeay-Bizot for the class CS-510_